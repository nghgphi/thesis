                    @@@___THINGS TO DO___@@@
                        Cố lên Phi ơi

1. Design Network
    1.1 Adjust activation function 
    1.2 Arrange it bwt ConV + Linear layers
    1.3 Consider in case batch_norm.weight has requires_grad=True/False
    
    1.4 vars in lenet & lenet_spectral
        lenet
            name: conv1.weight, p: torch.Size([20, 3, 5, 5])
            name: bn1.weight, p: torch.Size([20])
            name: bn1.bias, p: torch.Size([20])
            name: conv2.weight, p: torch.Size([50, 20, 5, 5])
            name: bn2.weight, p: torch.Size([50])
            name: bn2.bias, p: torch.Size([50])
            name: fc1.weight, p: torch.Size([800, 3200])
            name: fc2.weight, p: torch.Size([500, 800])
            name: head.weight, p: torch.Size([100, 500])
        lenet_spectral
            name: conv1.log_lipschitz
            name: conv1.conv.parametrizations.weight.original
            name: conv2.log_lipschitz
            name: conv2.conv.parametrizations.weight.original
            name: fc1.weight;
            name: fc2.weight
            name: head.weight
    1.5 Consider in case not using freeze_bn
2. Design Loss function with grad_norm + curvature regularization
    2.1 Consider whether it's necessary to use register_forward_hook()
    
3. Update optimizer in SAM model
    - lenet_spectral: Total parameters need to be updated:3019543
    - lenet: Total parameters need to be updated:3036640



